{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKmHf_etYoop"
   },
   "source": [
    "#FlipFeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlKdGeWYYvW8"
   },
   "source": [
    "###Objetivo:\n",
    "> Identificar cuando una persona realiza una dominada correctamente\n",
    "\n",
    "###Metodo:\n",
    "> Usaremos en este caso, redes neuronales recurrentes para obtener un valor booleano\n",
    "\n",
    "###Herramientas:\n",
    "\n",
    "0.   Python\n",
    "1.   Keras - TensorFLow\n",
    "2.   Yolo v8 - entrenado con un data set personalizado\n",
    "3.   OpenCV optical flow\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUamA1M62MrK"
   },
   "source": [
    "####Montando el directorio del drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoN8nkBzmMZC",
    "outputId": "60395e08-3764-4408-9bfe-7e805b6e3dae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ne6BoWRPA3qU"
   },
   "outputs": [],
   "source": [
    "global dirFlipfeed, dataSetPath, dirTrV, dirOut\n",
    "dirFlipfeed = '/content/drive/Shareddrives/flipfeed'\n",
    "dataSetPath  =  '/content/drive/Shareddrives/flipfeed/dataSetToTraining/ballAndPerson'\n",
    "dirTrV = './drive/Shareddrives/flipfeed/TrainingVideos'\n",
    "dirOut = './drive/Shareddrives/flipfeed/Output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2rxHWTub5x-"
   },
   "source": [
    "##Yolo v8 y OpenCV Optical Flow\n",
    "Yolov8 lo utilizamos para etiquetar los objetos en el video. Lo entrenamos con un data set custom, podiendo asÃ­ extraer las coordenadas en donde se encuentra la persona y el balÃ³n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHkeeq0c8ox1"
   },
   "source": [
    "#### ðŸ‘‡ Instalamos todos los paquetes necesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ny_QV-_ldE_R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\python311\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (1.24.2)\n",
      "Requirement already satisfied: opencv-python in c:\\python311\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: ultralytics in c:\\python311\\lib\\site-packages (8.0.112)\n",
      "Requirement already satisfied: supervision==0.1.0 in c:\\python311\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from torch) (3.10.7)\n",
      "Requirement already satisfied: typing-extensions in c:\\python311\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\python311\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\python311\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in c:\\python311\\lib\\site-packages (from ultralytics) (3.7.0)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\python311\\lib\\site-packages (from ultralytics) (9.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\python311\\lib\\site-packages (from ultralytics) (6.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\python311\\lib\\site-packages (from ultralytics) (2.28.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\python311\\lib\\site-packages (from ultralytics) (1.10.1)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in c:\\python311\\lib\\site-packages (from ultralytics) (0.15.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\python311\\lib\\site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\python311\\lib\\site-packages (from ultralytics) (1.5.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\python311\\lib\\site-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: psutil in c:\\python311\\lib\\site-packages (from ultralytics) (5.9.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python311\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python311\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python311\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python311\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python311\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2022.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python311\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch numpy opencv-python ultralytics supervision==0.1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YtLs4oEEd0dK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING  Ultralytics settings reset to defaults. This is normal and may be due to a recent ultralytics package update, but may have overwritten previous settings. \n",
      "View and update settings with 'yolo settings' or at 'C:\\Users\\Striwey\\AppData\\Roaming\\Ultralytics\\settings.yaml'\n"
     ]
    }
   ],
   "source": [
    "# Import the necesary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from time import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from supervision.draw.color import ColorPalette\n",
    "from supervision.tools.detections import Detections, BoxAnnotator\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_Qys3ac6mGG"
   },
   "source": [
    " #### ðŸ‘‰ Entrenando Yolo v8 small con un data set custom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRxA9y5G7W_W",
    "outputId": "9023e403-4dd1-41af-8d09-3a56b1704793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to yolov8s.pt...\n",
      "100% 21.5M/21.5M [00:00<00:00, 32.8MB/s]\n",
      "Ultralytics YOLOv8.0.112 ðŸš€ Python-3.10.11 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/content/drive/Shareddrives/flipfeed/dataSetToTraining/ballAndPerson/data.yaml, epochs=25, patience=50, batch=-1, imgsz=1599, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
      "100% 755k/755k [00:00<00:00, 13.7MB/s]\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  ultralytics.nn.modules.head.Detect           [2, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11136374 parameters, 11136358 gradients\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to yolov8n.pt...\n",
      "100% 6.23M/6.23M [00:00<00:00, 44.7MB/s]\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "WARNING âš ï¸ imgsz=[1599] must be multiple of max stride 32, updating to [1600]\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=1600\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (Tesla T4) 14.75G total, 0.25G reserved, 0.12G allocated, 14.38G free\n",
      "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n",
      "    11136374           0         1.699         203.8         363.2      (1, 3, 1600, 1600)                    list\n",
      "    11136374           0         3.087         92.68         130.8      (2, 3, 1600, 1600)                    list\n",
      "    11136374           0         6.520         129.4         207.7      (4, 3, 1600, 1600)                    list\n",
      "    11136374           0        12.604         206.7           392      (8, 3, 1600, 1600)                    list\n",
      "CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 14.75 GiB total capacity; 13.06 GiB already allocated; 32.81 MiB free; 13.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 6 for CUDA:0 9.88G/14.75G (67%) âœ…\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.000515625), 63 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/Shareddrives/flipfeed/dataSetToTraining/ballAndPerson/train/labels.cache... 1119 images, 0 backgrounds, 0 corrupt: 100% 1119/1119 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/Shareddrives/flipfeed/dataSetToTraining/ballAndPerson/valid/labels.cache... 320 images, 0 backgrounds, 0 corrupt: 100% 320/320 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "Image sizes 1600 train, 1600 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 25 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/25       9.1G      1.941      3.373      2.046         10       1600: 100% 187/187 [02:24<00:00,  1.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [01:38<00:00,  3.65s/it]\n",
      "                   all        320       1103      0.294      0.357      0.245      0.126\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/25      9.45G      1.803      2.326      1.843         21       1600: 100% 187/187 [02:07<00:00,  1.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:15<00:00,  1.77it/s]\n",
      "                   all        320       1103      0.341      0.293      0.237      0.118\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/25       9.5G      1.802       2.42      1.825         21       1600: 100% 187/187 [02:09<00:00,  1.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:14<00:00,  1.80it/s]\n",
      "                   all        320       1103      0.331      0.231      0.186     0.0942\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/25      9.48G      1.834      2.514      1.871         20       1600: 100% 187/187 [02:07<00:00,  1.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:15<00:00,  1.78it/s]\n",
      "                   all        320       1103      0.315      0.309      0.224      0.112\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/25      9.55G      1.835      2.469      1.878          7       1600: 100% 187/187 [02:07<00:00,  1.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:15<00:00,  1.76it/s]\n",
      "                   all        320       1103      0.306      0.231      0.176     0.0852\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/25      9.48G       1.84      2.411       1.91         21       1600: 100% 187/187 [02:05<00:00,  1.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:14<00:00,  1.81it/s]\n",
      "                   all        320       1103      0.387      0.307      0.277      0.143\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/25      9.05G      1.795       2.37      1.886         21       1600: 100% 187/187 [02:04<00:00,  1.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:15<00:00,  1.74it/s]\n",
      "                   all        320       1103      0.406       0.32      0.271      0.136\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/25      9.53G      1.766      2.327      1.862         18       1600: 100% 187/187 [02:04<00:00,  1.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:15<00:00,  1.78it/s]\n",
      "                   all        320       1103      0.387      0.336      0.297      0.156\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/25       9.5G      1.735      2.252      1.844         13       1600: 100% 187/187 [02:07<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:15<00:00,  1.72it/s]\n",
      "                   all        320       1103      0.396      0.338       0.31      0.173\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/25      9.45G       1.73      2.196      1.822         10       1600: 100% 187/187 [02:03<00:00,  1.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:15<00:00,  1.80it/s]\n",
      "                   all        320       1103      0.383      0.409      0.325      0.177\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/25      9.47G      1.679      2.097      1.788         23       1600: 100% 187/187 [02:04<00:00,  1.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:14<00:00,  1.83it/s]\n",
      "                   all        320       1103      0.455       0.34       0.33      0.178\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/25      8.68G      1.677      2.032       1.78         19       1600: 100% 187/187 [02:04<00:00,  1.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:15<00:00,  1.79it/s]\n",
      "                   all        320       1103      0.415      0.374      0.354      0.195\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/25      9.51G      1.615      1.963      1.753          6       1600: 100% 187/187 [02:02<00:00,  1.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:14<00:00,  1.85it/s]\n",
      "                   all        320       1103      0.487      0.402       0.38      0.196\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/25      9.06G      1.616      1.974      1.744          7       1600: 100% 187/187 [02:01<00:00,  1.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:14<00:00,  1.83it/s]\n",
      "                   all        320       1103      0.491      0.373      0.375      0.205\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/25      9.37G      1.606      1.895      1.725          3       1600: 100% 187/187 [02:00<00:00,  1.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:14<00:00,  1.82it/s]\n",
      "                   all        320       1103      0.484       0.37      0.374      0.199\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/25      9.37G      1.563       1.81      1.696          6       1600: 100% 187/187 [02:03<00:00,  1.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:15<00:00,  1.77it/s]\n",
      "                   all        320       1103      0.462       0.39      0.351      0.199\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/25      9.45G       1.57       1.81      1.697          9       1600: 100% 187/187 [02:02<00:00,  1.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:14<00:00,  1.82it/s]\n",
      "                   all        320       1103       0.44      0.403      0.381      0.198\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/25      9.55G      1.522      1.773      1.671         13       1600: 100% 187/187 [02:02<00:00,  1.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:14<00:00,  1.83it/s]\n",
      "                   all        320       1103      0.513      0.415      0.398      0.219\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/25      9.44G      1.508      1.726      1.656          2       1600: 100% 187/187 [02:00<00:00,  1.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:14<00:00,  1.80it/s]\n",
      "                   all        320       1103      0.523      0.426      0.397      0.218\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/25      9.54G      1.476      1.612      1.612         23       1600: 100% 187/187 [02:02<00:00,  1.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:15<00:00,  1.77it/s]\n",
      "                   all        320       1103      0.489      0.454      0.431       0.23\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/25      9.54G      1.454        1.6      1.616         16       1600: 100% 187/187 [02:04<00:00,  1.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:14<00:00,  1.81it/s]\n",
      "                   all        320       1103      0.467      0.419      0.391      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/25      9.45G      1.464      1.567      1.613         15       1600: 100% 187/187 [02:04<00:00,  1.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:14<00:00,  1.82it/s]\n",
      "                   all        320       1103      0.519      0.433       0.42      0.229\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/25      9.43G      1.429      1.477      1.581          3       1600: 100% 187/187 [02:03<00:00,  1.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:15<00:00,  1.78it/s]\n",
      "                   all        320       1103      0.553      0.432      0.437      0.246\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/25      9.44G      1.393      1.444      1.565         15       1600: 100% 187/187 [02:04<00:00,  1.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:14<00:00,  1.84it/s]\n",
      "                   all        320       1103      0.561      0.427      0.446      0.245\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/25      9.47G      1.368      1.428      1.541         17       1600: 100% 187/187 [02:04<00:00,  1.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:23<00:00,  1.15it/s]\n",
      "                   all        320       1103      0.531      0.458       0.44      0.243\n",
      "\n",
      "25 epochs completed in 1.006 hours.\n",
      "Optimizer stripped from runs/detect/train/weights/last.pt, 22.8MB\n",
      "Optimizer stripped from runs/detect/train/weights/best.pt, 22.8MB\n",
      "\n",
      "Validating runs/detect/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.112 ðŸš€ Python-3.10.11 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
      "Model summary (fused): 168 layers, 11126358 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 27/27 [00:20<00:00,  1.32it/s]\n",
      "                   all        320       1103       0.56      0.426      0.446      0.246\n",
      "                  Ball        320        304      0.609      0.559      0.562      0.351\n",
      "                Person        320        799       0.51      0.293      0.329       0.14\n",
      "Speed: 8.6ms preprocess, 32.9ms inference, 0.0ms loss, 4.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=train model=yolov8s.pt data={dataSetPath}/data.yaml epochs=25 imgsz=1599 plots=True batch=-1\n",
    "# Data set custom extracted from (url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zs3Dhqt2m0P"
   },
   "source": [
    "#### ðŸ‘ Validando el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wX8MSDsSEIl5",
    "outputId": "a0180f7d-4a88-467a-d184-8e51e7e4c1d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.112 ðŸš€ Python-3.10.11 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
      "Model summary (fused): 168 layers, 11126358 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/Shareddrives/flipfeed/dataSetToTraining/ballAndPerson/valid/labels.cache... 320 images, 0 backgrounds, 0 corrupt: 100% 320/320 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 20/20 [00:40<00:00,  2.05s/it]\n",
      "                   all        320       1103      0.561      0.426      0.446      0.246\n",
      "                  Ball        320        304      0.609      0.559      0.563      0.353\n",
      "                Person        320        799      0.513      0.293      0.329       0.14\n",
      "Speed: 4.5ms preprocess, 50.2ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=val model={dirFlipfeed}/runs/detect/train/weights/best.pt data={dataSetPath}/data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ChDLgxd9EtF"
   },
   "outputs": [],
   "source": [
    "#To inference\n",
    "!yolo task=detect mode=predict model={dirFlipfeed}/runs/detect/train/weights/best.pt conf=0.25 source={dataSetPath}/test/images save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fMyu-cID-5i8"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() missing 1 required positional argument: 'frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ret\n\u001b[1;32m---> 57\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_bboxes(results, frame)\n\u001b[0;32m     60\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time()\n",
      "\u001b[1;31mTypeError\u001b[0m: predict() missing 1 required positional argument: 'frame'"
     ]
    }
   ],
   "source": [
    "def predict(self, frame):\n",
    "       \n",
    "    model = YOLO('./runs/detect/train/weights/best.pt')\n",
    "    results = model.predict(frame)\n",
    "    \n",
    "    return results\n",
    "    \n",
    "\n",
    "def plot_bboxes(self, results, frame):\n",
    "\n",
    "    xyxys = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    # Extract detections for person class\n",
    "    for result in results[0]:\n",
    "        class_id = result.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "        if class_id == 0:\n",
    "\n",
    "            xyxys.append(result.boxes.xyxy.cpu().numpy())\n",
    "            confidences.append(result.boxes.conf.cpu().numpy())\n",
    "            class_ids.append(result.boxes.cls.cpu().numpy().astype(int))\n",
    "\n",
    "\n",
    "    # Setup detections for visualization\n",
    "    detections = Detections(\n",
    "                xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
    "                confidence=results[0].boxes.conf.cpu().numpy(),\n",
    "                class_id=results[0].boxes.cls.cpu().numpy().astype(int),\n",
    "                )\n",
    "\n",
    "\n",
    "    # Format custom labels\n",
    "    labels = [f\"{self.CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n",
    "    for _, confidence, class_id, tracker_id\n",
    "    in detections]\n",
    "\n",
    "    # Annotate and display frame\n",
    "    frame = self.box_annotator.annotate(frame=frame, detections=detections, labels=labels)\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "assert cap.isOpened()\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "while True:\n",
    "          \n",
    "    start_time = time()\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    assert ret\n",
    "\n",
    "    results = predict(frame)\n",
    "    frame = self.plot_bboxes(results, frame)\n",
    "\n",
    "    end_time = time()\n",
    "    fps = 1/np.round(end_time - start_time, 2)\n",
    "\n",
    "    cv2.putText(frame, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow('YOLOv8 Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3alb5EWiNgC"
   },
   "source": [
    "##OpenCV Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBI7KN8xiMYk"
   },
   "outputs": [],
   "source": [
    "#To draw the lines on the frame\n",
    "def draw_flow(img, flow, step=20):\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2,-1).astype(int)\n",
    "    fx, fy = flow[y,x].T\n",
    "\n",
    "    lines = np.vstack([x, y, x-fx, y-fy]).T.reshape(-1, 2, 2)\n",
    "    lines = np.int32(lines + 0.5)\n",
    "\n",
    "    img_bgr = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.polylines(img_bgr, lines, 0, (0, 255, 0))\n",
    "\n",
    "    for (x1, y1), (_x2, _y2) in lines:\n",
    "        cv2.circle(img_bgr, (x1, y1), 1, (0, 255, 0), -1)\n",
    "\n",
    "    return img_bgr\n",
    "\n",
    "#Capture the video and generate a output with cv2\n",
    "videoPath = 'dom2.mp4'\n",
    "#a = open(dirTrV + videoPath, 'r')\n",
    "input = cv2.VideoCapture(dirTrV + '/'+videoPath)\n",
    "output = cv2.VideoWriter(dirOut + '/output_' + videoPath, cv2.VideoWriter_fourcc(*'mp4v'), 10, (int(input.get(3)),int(input.get(4))))\n",
    "\n",
    "suc, prev = input.read()\n",
    "prevgray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while (input.isOpened()):\n",
    "    #Extract a frame from video and put on it a B&W format \n",
    "    suc, img = input.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #Obtain the motion prediction\n",
    "    flow = cv2.calcOpticalFlowFarneback(prevgray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    prevgray = gray\n",
    "\n",
    "    #Write frames on output\n",
    "    imgToShow = draw_flow(gray, flow)\n",
    "    output.write(imgToShow)\n",
    "\n",
    "output.release()\n",
    "input.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_c5nnt4wG0UH"
   },
   "source": [
    "##RNN implement with Keras - Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKXLYxSBHBG_",
    "outputId": "f5c78c5c-0a26-4086-a199-c1f1b5604cb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as kr\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "lr = 0.01           # learning rate\n",
    "nn = [2, 16, 8, 1]  # nÃºmero de neuronas por capa.\n",
    "\n",
    "model = kr.Sequential()\n",
    "\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "model.add(layers.SimpleRNN(128))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compilamos el modelo, definiendo la funciÃ³n de coste y el optimizador.\n",
    "#model.compile(loss='mse', optimizer=kr.optimizers.SGD(lr=0.05), metrics=['acc'])\n",
    "\n",
    "# Y entrenamos al modelo. Los callbacks \n",
    "#model.fit(X, Y, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
