{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#FlipFeed"
      ],
      "metadata": {
        "id": "cKmHf_etYoop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Objetivo:\n",
        "> Identificar cuando una persona realiza una dominada correctamente\n",
        "\n",
        "###Metodo:\n",
        "> Usaremos en este caso, redes neuronales recurrentes para obtener un valor booleano\n",
        "\n",
        "###Herramientas:\n",
        "\n",
        "0.   Python\n",
        "1.   Keras - TensorFLow\n",
        "2.   Yolo v8 - entrenado con un data set personalizado\n",
        "3.   OpenCV optical flow\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BlKdGeWYYvW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Montando el directorio del drive"
      ],
      "metadata": {
        "id": "VUamA1M62MrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "global dirTrV, dirOut\n",
        "dirTrV = './drive/Shareddrives/flipfeed/TrainingVideos'\n",
        "dirOut = './drive/Shareddrives/flipfeed/Output'"
      ],
      "metadata": {
        "id": "CoN8nkBzmMZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3fdc1c6-4637-4097-9db0-94fc90344872"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Yolo v8 y OpenCV Optical Flow\n",
        "Yolov8 lo utilizamos para etiquetar los objetos en el video, podiendo así extraer las coordenadas en donde se encuentra la persona y el balón.\n",
        "Acontinuación instalamos todos los paquetes necesarios."
      ],
      "metadata": {
        "id": "Y2rxHWTub5x-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy opencv-python ultralytics supervision==0.1.0 "
      ],
      "metadata": {
        "id": "ny_QV-_ldE_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necesary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from time import time\n",
        "from ultralytics import YOLO\n",
        "\n",
        "from supervision.draw.color import ColorPalette\n",
        "from supervision.tools.detections import Detections, BoxAnnotator"
      ],
      "metadata": {
        "id": "YtLs4oEEd0dK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##OpenCV Optical Flow"
      ],
      "metadata": {
        "id": "_3alb5EWiNgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To draw the lines on the frame\n",
        "def draw_flow(img, flow, step=20):\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2,-1).astype(int)\n",
        "    fx, fy = flow[y,x].T\n",
        "\n",
        "    lines = np.vstack([x, y, x-fx, y-fy]).T.reshape(-1, 2, 2)\n",
        "    lines = np.int32(lines + 0.5)\n",
        "\n",
        "    img_bgr = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "    cv2.polylines(img_bgr, lines, 0, (0, 255, 0))\n",
        "\n",
        "    for (x1, y1), (_x2, _y2) in lines:\n",
        "        cv2.circle(img_bgr, (x1, y1), 1, (0, 255, 0), -1)\n",
        "\n",
        "    return img_bgr\n",
        "\n",
        "#Capture the video and generate a output with cv2\n",
        "videoPath = 'dom2.mp4'\n",
        "#a = open(dirTrV + videoPath, 'r')\n",
        "input = cv2.VideoCapture(dirTrV + '/'+videoPath)\n",
        "output = cv2.VideoWriter(dirOut + '/output_' + videoPath, cv2.VideoWriter_fourcc(*'mp4v'), 10, (int(input.get(3)),int(input.get(4))))\n",
        "\n",
        "suc, prev = input.read()\n",
        "prevgray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "while (input.isOpened()):\n",
        "    #Extract a frame from video and put on it a B&W format \n",
        "    suc, img = input.read()\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    #Obtain the motion prediction\n",
        "    flow = cv2.calcOpticalFlowFarneback(prevgray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "    prevgray = gray\n",
        "\n",
        "    #Write frames on output\n",
        "    imgToShow = draw_flow(gray, flow)\n",
        "    output.write(imgToShow)\n",
        "\n",
        "output.release()\n",
        "input.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "gBI7KN8xiMYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RNN implement with Keras - Tensorflow"
      ],
      "metadata": {
        "id": "_c5nnt4wG0UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as kr\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "lr = 0.01           # learning rate\n",
        "nn = [2, 16, 8, 1]  # número de neuronas por capa.\n",
        "\n",
        "model = kr.Sequential()\n",
        "\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "model.add(layers.SimpleRNN(128))\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Compilamos el modelo, definiendo la función de coste y el optimizador.\n",
        "#model.compile(loss='mse', optimizer=kr.optimizers.SGD(lr=0.05), metrics=['acc'])\n",
        "\n",
        "# Y entrenamos al modelo. Los callbacks \n",
        "#model.fit(X, Y, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKXLYxSBHBG_",
        "outputId": "f5c78c5c-0a26-4086-a199-c1f1b5604cb3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        }
      ]
    }
  ]
}